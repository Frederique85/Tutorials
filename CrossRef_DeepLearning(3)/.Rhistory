until_print_pub_date="2020-12-31",
type=c("journal-article","proceedings-article")),
limit= 20, .progress="text")
DL <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="journal-article"),
limit= 20, .progress="text")
DL_articles <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="journal-article"),
limit= 20, .progress="text")
DL_proceedings <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="proceedings-article"),
limit= 20, .progress="text")
View(DL_articles)
DL_articles$meta$total_results
DL_proceedings$meta$total_results
DL_proceedings <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="proceedings-article"),
cursor = "*", cursor_max = 25742, .progress="text")
names(DL_data)
View(DL_proceedings)
saveRDS("DL_proceedings")
saveRDS(DL_proceedings$data)
proceedings <- DL_proceedings$data
saveRDS(proceedings)
proceedings <- DL_proceedings$data
save_RDS(proceedings, "proceedings.RDS")
saveRDS(proceedings, "proceedings.RDS")
rm(DL_proceedings)
names(DL_data)
DL_articles <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="journal-article"),
limit= 20, .progress="text")
View(DL_articles)
DL_articles <- cr_works(query="deep learning",
filter=c(from_print_pub_date="2020-01-01",
until_print_pub_date="2020-12-31",
type="journal-article"),
cursor = "*", cursor_max = 62200, .progress="text")
DL_articles <- DL_articles$data
names(DL_articles)                      # View the structure of each dataset
names(DL_proceedings)
DL_proceedings <- proceedings
names(DL_articles)                      # View the structure of each dataset
names(DL_proceedings)
DL <- bind_rows(DL_articles, DL_proceedings)
View(DL)
View(DL_articles)
saveRDS(DL, "DeepLearning_dataset.RDS")
## CrossRef Deep learning tutorial (2)
##  by F. Bone / www.brightdatasci.com
##  on 20/06/2021
#############################################
#############################################
## Summary of the tutorial
#----------------------------
#(1) Load the relevant and dataset
#(2) Explore the data to extract the relevant variables
#(3) Keep only the records with deep learning
#(4) Clean the dataset and extract funding
# 1. Load the relevant libraries and dataset
#--------------------------------------------
library(tidyverse)          # load the library needed
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # Point to the folder where this file is saved
DL <- read_rds("DeepLearning_dataset.RDS")                         # Load the file from the previous tutorial
# 2. Explore the data to extract the relevant variables
#-------------------------------------------------------
sum(is.na(DL$doi))          # Check whether DOIs is a good identifier
length(unique(DL$doi))
View(DL)
library(tidyverse)          # load the library needed
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # Point to the folder where this file is saved
DL <- read_rds("DeepLearning_dataset.RDS")                         # Load the file from the previous tutorial
# 2. Explore the data to extract the relevant variables
#-------------------------------------------------------
sum(is.na(DL$doi))          # Check whether DOIs is a good identifier
length(unique(DL$doi))
DL <- DL %>%                # keep only a select number of columns
select(doi, type, author, funder, abstract, title)
# 3. Keep only the records with deep learning
#---------------------------------------------
DL <- DL %>%
filter(grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(title),
perl=TRUE)
|
grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(abstract),
perl=TRUE))
DL <- DL %>%
select(- title, -abstract)
which(colnames(DL_authors)=="affiliation.name")
ncol(DL_authors)
DL_authors <- DL %>%
select(-funder)%>%
unnest(author)                  # Extract authors
# Check the column number
which(colnames(DL_authors)=="affiliation.name")
ncol(DL_authors)
# Check author names with ORCID identifier
sum(is.na(DL_authors2$ORCID))/nrow(DL_authors2)
# Check affiliations
sum(""==(DL_authors2$affiliation))/nrow(DL_authors2)
# Merge all affiliations into one - semicolon
DL_authors <- DL_authors %>%
unite(affiliation, affiliation.name:affiliation5.name, sep = ";", na.rm = TRUE, remove=TRUE ) %>%
separate_rows(affiliation, sep = ";") %>%
filter(!is.na(given) & !is.na(family))%>%
rename(forename = given)%>%
rename(familyname = family)
# Check author names with ORCID identifier
sum(is.na(DL_authors$ORCID))/nrow(DL_authors)
# Check affiliations
sum(""==(DL_authors$affiliation))/nrow(DL_authors)
View(DL_authors)
DL_funder <- DL %>%
select(doi, funder)%>%
unnest(funder)
View(DL_funder)
DL_funder <- DL_funder %>%
select(doi, DOI, name) %>%
rename(doi.funder = DOI) %>%
unique()
View(DL_funder)
# Check how many papers had funding information
length(unique(DL_funder$doi))/nrow(DL)
# Check how many of the funder extract actually have an unique identifier
sum(!is.na(DL_funder$doi.funder))/nrow(DL_funder)
# 6. Improve the funder's doi matching
#---------------------------------------------
# Split the dataset into two parts one which has dois and the other which does not
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder))
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
View(DL_funder_nodoi)
View(thesaurus_doi)
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder))
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi.funder) %>%
unique()
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder))
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi.funder) %>%
unique()
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder)) %>%
select(-doi.funder)
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
View(DL_funder_nodoi)
DL_funder_doi2 <- DL_funder_nodoi %>%
filter(!is.na(doi.funder))
DL_funder_nodoi_summary <- DL_funder_nodoi %>%
filter(is.na(doi.funder))  %>%
group_by(funder_name)%>%
summarise(count=n())
DL_funder <- DL %>%
select(doi, funder)%>%
unnest(funder)
# Clean funder data
DL_funder <- DL_funder %>%
select(doi, DOI, name) %>%
rename(doi.funder = DOI) %>%
rename(funder.name = name) %>%
unique()
# Check how many papers had funding information
length(unique(DL_funder$doi))/nrow(DL)
# Check how many of the funder extract actually have an unique identifier
sum(!is.na(DL_funder$doi.funder))/nrow(DL_funder)
# 6. Improve the funder's doi matching
#---------------------------------------------
# Split the dataset into two parts one which has dois and the other which does not
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder)) %>%
select(-doi.funder)
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
DL_funder_doi2 <- DL_funder_nodoi %>%
filter(!is.na(doi.funder))
DL_funder_nodoi_summary <- DL_funder_nodoi %>%
filter(is.na(doi.funder))  %>%
group_by(funder.name)%>%
summarise(count=n())
View(DL_funder_nodoi_summary)
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)
DL_funder_doi_summary <- DL_funder_doi %>%
group_by(doi.funder, funder.name)%>%
summarise(count=n())
View(DL_funder_doi_summary)
write_csv(DL_funder_nodoi_summary, "DL_funder_nodoi.csv")
DL_funder_manual <- read_csv("DL_funder_nodoi_manual.csv")
View(DL_funder_manual)
View(DL_funder_manual)
# keep only the ones with ids
DL_funder_manual <- DL_funder_manual %>%
filter(!is.na(funder.name))%>%
select(funder.name, doi.funder)
# keep only the ones with ids
DL_funder_manual <- DL_funder_manual %>%
filter(!is.na(doi.funder))%>%
select(funder.name, doi.funder)
DL_funder_doi2 <- left_join(DL_funder_nodoi, DL_funder_manual)%>%
filter(!is.na(doi.funder))
View(DL_funder_doi2)
# Merge with the dataset with dois
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)
?summarise
#
DL_funder_summary <- DL_funder_doi  %>%
group_by(funder_name, doi_name)%>%
summarise(count=n()) %>%
filter(count > 5)
#
DL_funder_summary <- DL_funder_doi  %>%
group_by(funder.name, doi.name)%>%
summarise(count=n()) %>%
filter(count > 5)
#
DL_funder_summary <- DL_funder_doi  %>%
group_by(funder.name, doi.funder)%>%
summarise(count=n()) %>%
filter(count > 5)
View(DL_funder_summary)
DL_funder_summary <- DL_funder_doi  %>%
group_by(funder.name, doi.funder)%>%
summarise(count=n()) %>%
filter(count > 4)
DL_funder_summary <- DL_funder_doi  %>%
group_by(doi.funder)%>%
summarise(count=n()) %>%
filter(count > 4)
library(jsonlite)
View(DL_funder_doi)
# Reduce the number of funders to one that have 5 occurrences of more
DL_funder_summary <- DL_funder_doi  %>%
group_by(doi.funder) %>%
summarise(count=n()) %>%
filter(count > 4) %>%
ungroup() %>%
arrange(desc(count))
# Remove the count
DL_funder_summary <- DL_funder_summary %>%
select(-count)
# Select the rows in the dataset with the funders we are interested in
DL_funder <- right_join(DL_funder, DL_funder_summary)
View(DL_funder_doi)
DL_funder_doi <- right_join(DL_funder_doi, DL_funder_summary)
View(DL_funder_doi)
# Split the dataset into two parts one which has dois and the other which does not
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder)) %>%
select(-doi.funder)
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
# Join the dataset with no doi with the thesaurus with dois
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
DL_funder_doi2 <- DL_funder_nodoi %>%           # Make a dataset with new matches
filter(!is.na(doi.funder))
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)     # add the new matches to the doi dataset
DL_funder_nodoi_summary <- DL_funder_nodoi %>%               # check for the most frequent names without a doi
filter(is.na(doi.funder))  %>%
group_by(funder.name)%>%
summarise(count=n())
# Export the summary to add manually dois using the first dataset.
write_csv(DL_funder_nodoi_summary, "DL_funder_nodoi.csv")
# import the manually checked dataset
DL_funder_manual <- read_csv("DL_funder_nodoi_manual.csv")
# keep only the ones with ids
DL_funder_manual <- DL_funder_manual %>%
filter(!is.na(doi.funder))%>%
select(funder.name, doi.funder)
# Get the original id of papers back
DL_funder_doi2 <- left_join(DL_funder_nodoi, DL_funder_manual)%>%
filter(!is.na(doi.funder))
# Merge with the dataset with dois
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)
DL_funder_summary <- DL_funder_doi  %>%
group_by(doi.funder) %>%
summarise(count=n()) %>%
filter(count > 4) %>%
ungroup() %>%
arrange(desc(count))
# Remove the count
DL_funder_summary <- DL_funder_summary %>%
select(-count)
View(DL_funder)
## Summary of the tutorial
#----------------------------
#(1) Load the relevant and dataset
#(2) Explore the data to extract the relevant variables
#(3) Keep only the records with deep learning
#(4) Explore the authorship data
#(5) Explore the funding data
#(6) Improve the funder's doi matching
#(7) Finalise the dataset
# 1. Load the relevant libraries and dataset
#--------------------------------------------
library(tidyverse)          # load the library needed
library(jsonlite)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # Point to the folder where this file is saved
DL <- read_rds("DeepLearning_dataset.RDS")                         # Load the file from the previous tutorial
# 2. Explore the data to extract the relevant variables
#-------------------------------------------------------
sum(is.na(DL$doi))          # Check whether DOIs is a good identifier
length(unique(DL$doi))
DL <- DL %>%                # keep only a select number of columns
select(doi, type, author, funder, abstract, title)
# 3. Keep only the records with deep learning
#---------------------------------------------
DL <- DL %>%
filter(grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(title),
perl=TRUE)
|
grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(abstract),
perl=TRUE))
DL <- DL %>%
select(- title, -abstract)
# 4. Clean the dataset and extract authors
#---------------------------------------------
DL_authors <- DL %>%
select(-funder)%>%
unnest(author)                  # Extract authors
# Merge all affiliations into one - semicolon
DL_authors <- DL_authors %>%
unite(affiliation, affiliation.name:affiliation5.name, sep = ";", na.rm = TRUE, remove=TRUE ) %>%
separate_rows(affiliation, sep = ";") %>%
filter(!is.na(given) & !is.na(family))%>%
rename(forename = given)%>%
rename(familyname = family)
# Check author names with ORCID identifier
sum(is.na(DL_authors$ORCID))/nrow(DL_authors)
# Check affiliations
sum(""==(DL_authors$affiliation))/nrow(DL_authors)
# 5. Explore the funding data
#---------------------------------------------
# make a dataset with funder data
DL_funder <- DL %>%
select(doi, funder)%>%
unnest(funder)
# Clean funder data
DL_funder <- DL_funder %>%
select(doi, DOI, name) %>%
rename(doi.funder = DOI) %>%
rename(funder.name = name) %>%
unique()
# Check how many papers had funding information
length(unique(DL_funder$doi))/nrow(DL)
# Check how many of the funder extract actually have an unique identifier
sum(!is.na(DL_funder$doi.funder))/nrow(DL_funder)
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder)) %>%
select(-doi.funder)
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
## CrossRef Deep learning tutorial (2)
##  by F. Bone / www.brightdatasci.com
##  on 20/06/2021
#############################################
#############################################
## Summary of the tutorial
#----------------------------
#(1) Load the relevant and dataset
#(2) Explore the data to extract the relevant variables
#(3) Keep only the records with deep learning
#(4) Explore the authorship data
#(5) Explore the funding data
#(6) Improve the funder's doi matching
#(7) Finalise the dataset
# 1. Load the relevant libraries and dataset
#--------------------------------------------
library(tidyverse)          # load the library needed
library(jsonlite)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))  # Point to the folder where this file is saved
DL <- read_rds("DeepLearning_dataset.RDS")                         # Load the file from the previous tutorial
# 2. Explore the data to extract the relevant variables
#-------------------------------------------------------
sum(is.na(DL$doi))          # Check whether DOIs is a good identifier
length(unique(DL$doi))
DL <- DL %>%                # keep only a select number of columns
select(doi, type, author, funder, abstract, title)
# 3. Keep only the records with deep learning
#---------------------------------------------
DL <- DL %>%
filter(grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(title),
perl=TRUE)
|
grepl ("\\bdeep\\b[[:space:]]*-*[\\w]*[[:space:]]*\\blearning\\b",
tolower(abstract),
perl=TRUE))
DL <- DL %>%
select(- title, -abstract)
# 4. Clean the dataset and extract authors
#---------------------------------------------
DL_authors <- DL %>%
select(-funder)%>%
unnest(author)                  # Extract authors
# Merge all affiliations into one - semicolon
DL_authors <- DL_authors %>%
unite(affiliation, affiliation.name:affiliation5.name, sep = ";", na.rm = TRUE, remove=TRUE ) %>%
separate_rows(affiliation, sep = ";") %>%
filter(!is.na(given) & !is.na(family))%>%
rename(forename = given)%>%
rename(familyname = family)
# Check author names with ORCID identifier
sum(is.na(DL_authors$ORCID))/nrow(DL_authors)
# Check affiliations
sum(""==(DL_authors$affiliation))/nrow(DL_authors)
# 5. Explore the funding data
#---------------------------------------------
# make a dataset with funder data
DL_funder <- DL %>%
select(doi, funder)%>%
unnest(funder)
# Clean funder data
DL_funder <- DL_funder %>%
select(doi, DOI, name) %>%
rename(doi.funder = DOI) %>%
rename(funder.name = name) %>%
unique()
# Check how many papers had funding information
length(unique(DL_funder$doi))/nrow(DL)
# Check how many of the funder extract actually have an unique identifier
sum(!is.na(DL_funder$doi.funder))/nrow(DL_funder)
# 6. Improve the funder's doi matching
#---------------------------------------------
# Split the dataset into two parts one which has dois and the other which does not
DL_funder_doi <- DL_funder %>%
filter(!is.na(doi.funder))
DL_funder_nodoi <- DL_funder %>%
filter(is.na(doi.funder)) %>%
select(-doi.funder)
# Make a thesaurus for funders with dois
thesaurus_doi <- DL_funder_doi %>%
select(-doi) %>%
unique()
# Join the dataset with no doi with the thesaurus with dois
DL_funder_nodoi <- left_join(DL_funder_nodoi, thesaurus_doi)
DL_funder_doi2 <- DL_funder_nodoi %>%           # Make a dataset with new matches
filter(!is.na(doi.funder))
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)     # add the new matches to the doi dataset
DL_funder_nodoi_summary <- DL_funder_nodoi %>%               # check for the most frequent names without a doi
filter(is.na(doi.funder))  %>%
group_by(funder.name)%>%
summarise(count=n())
# import the manually checked dataset
DL_funder_manual <- read_csv("DL_funder_nodoi_manual.csv")
# keep only the ones with ids
DL_funder_manual <- DL_funder_manual %>%
filter(!is.na(doi.funder))%>%
select(funder.name, doi.funder)
# Get the original id of papers back
DL_funder_doi2 <- left_join(DL_funder_nodoi, DL_funder_manual)%>%
filter(!is.na(doi.funder))
# Merge with the dataset with dois
DL_funder_doi <- bind_rows(DL_funder_doi, DL_funder_doi2)
DL_funder_summary <- DL_funder_doi  %>%
group_by(doi.funder) %>%
summarise(count=n()) %>%
filter(count > 4) %>%
ungroup() %>%
arrange(desc(count))
# Remove the count
DL_funder_summary <- DL_funder_summary %>%
select(-count)
View(DL_funder_summary)
DL_funder_doi2 <- right_join(DL_funder_doi, DL_funder_summary)
# Select the rows in the dataset with the funders we are interested in
DL_funder_doi <- right_join(DL_funder_doi, DL_funder_summary)
# Save both files
saveRDS(DL_funder_doi, "DeepLearning_article_funder.RDS") # Save the dataset, with funder data matched with publications
saveRDS(DL_funder_summary, "DeepLearning_funderdois.RDS") # Save the dataset with funders identifiers
